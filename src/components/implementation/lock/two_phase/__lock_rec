/**
 * Copyright 2008 by Boston University.  All rights reserved.
 *
 * Redistribution of this file is permitted under the GNU General
 * Public License v2.
 *
 * Initial author: Gabriel Parmer, gabep1@cs.bu.edu, 2008.
 */

#define COS_FMT_PRINT

//#include <cos_synchronization.h>
#include <cos_component.h>
#include <cos_alloc.h>
#include <cos_debug.h>
#include <cos_list.h>
#include <print.h>
#include <cos_vect.h>

#include <objtype.h>

#include <lock.h>
#include <ck_spinlock.h>

#include <sched.h>

#if (RECOVERY_ENABLE == 1)
#include <c3_test.h>
#endif
unsigned int swifi_ready = 0;  // explicitly initialize to 0

static int test_num = 0;

//#define ACT_LOG
#ifdef ACT_LOG
#define ACT_LOG_LEN 32
#define ACTION_TIMESTAMP 1

typedef enum {
	ACT_PRELOCK,
	ACT_LOCK,
	ACT_UNLOCK,
	ACT_WAKE,
	ACT_WAKEUP
} action_t;
typedef enum {
	ACT_SPDID,
	ACT_LOCK_ID,
	ACT_T1,
	ACT_T2,
	ACT_ITEM_MAX
} action_item_t;
#define NUM_ACT_ITEMS ACT_ITEM_MAX

#include <cos_actlog.h>
#define ACT_RECORD(a, s, l, t1, t2)					\
	do {								\
		unsigned long as[] = {s, l, t1, t2};			\
		action_record(a, as, NULL);				\
	} while (0)
#else
#define ACT_RECORD(a, s, l, t1, t2)
#endif

struct blocked_thds {
	unsigned short int thd_id;
	struct blocked_thds *next, *prev;
};

struct meta_lock {
	u16_t owner;
	spdid_t spd;
	unsigned long lock_id;
	struct blocked_thds b_thds;
	unsigned long long gen_num;

	struct meta_lock *next, *prev;
};


static volatile unsigned long lock_id = 1;
/* Head of the linked list of locks. */
static struct meta_lock STATIC_INIT_LIST(locks, next, prev);
static volatile unsigned long long generation = 0;
/* Datastructure of blocked thread structures */
COS_VECT_CREATE_STATIC(bthds);

#if NUM_CPU_COS > 1
ck_spinlock_ticket_t xcore_lock = CK_SPINLOCK_TICKET_INITIALIZER;
#define TAKE(spdid) 	do { if (sched_component_take(spdid)) BUG(); ck_spinlock_ticket_lock_pb(&xcore_lock, 1); } while (0)
#define RELEASE(spdid)	do { ck_spinlock_ticket_unlock(&xcore_lock); if (sched_component_release(spdid)) BUG();  } while (0)
#else
#define TAKE(spdid)							\
	/* lock take start	*/					\
	do { if (sched_component_take(cos_spd_id()))    BUG(); } while (0) \
		/* lock take end */					\
		
#define RELEASE(spdid)							\
	/* lock release start */					\
	do { if (sched_component_release(cos_spd_id())) BUG(); } while (0)	\
		/* lock release end */					\

#endif
/* 
 * FIXME: to make this predictable (avoid memory allocation in the
 * must-be-predictable case, we should really cos_vect_add_id when we
 * first find out about the possibility of the thread making any
 * invocations.
 */
static struct blocked_thds *bt_get(unsigned short int tid)
{
	struct blocked_thds *bt;

	bt = cos_vect_lookup(&bthds, tid);
	if (NULL == bt) {
		bt = malloc(sizeof(struct blocked_thds));
		if (NULL == bt) return NULL;
		INIT_LIST(bt, next, prev);
		bt->thd_id = tid;
		if (tid != cos_vect_add_id(&bthds, bt, tid)) return NULL;
	}
	return bt;
}

static inline struct meta_lock *lock_find(unsigned long lock_id, spdid_t spd)
{
	struct meta_lock *tmp;

	for (tmp = FIRST_LIST(&locks, next, prev) ; 
	     tmp != &locks ; 
	     tmp = FIRST_LIST(tmp, next, prev)) {
		if (tmp->lock_id == lock_id && tmp->spd == spd) {
			return tmp;
		}
		assert(tmp != FIRST_LIST(tmp, next, prev));
	}
	
	return NULL;
}

static void lock_print_all(void)
{
	struct meta_lock *ml;

	for (ml = FIRST_LIST(&locks, next, prev) ; 
	     ml != &locks ; // && ml != FIRST_LIST(ml, next, prev) ; 
	     ml = FIRST_LIST(ml, next, prev)) {
		printc("lock @ %p (next %p, prev %p), id %d, spdid %d\n", 
		       ml, ml->next, ml->prev, (unsigned int)ml->lock_id, ml->spd);
	}
	prints("\n");
}

static int lock_is_thd_blocked(struct meta_lock *ml, unsigned short int thd)
{
	struct blocked_thds *bt;

	for (bt = FIRST_LIST(&ml->b_thds, next, prev) ; bt != &ml->b_thds ; bt = bt->next) {
		if (bt->thd_id == thd) return 1;
	}
	return 0;
}

static struct meta_lock *lock_alloc(spdid_t spd)
{
	struct meta_lock *l;
	struct meta_lock *snd, *lst;

	l = (struct meta_lock*)malloc(sizeof(struct meta_lock));
	if (!l) return NULL;
	l->b_thds.thd_id = 0;
	INIT_LIST(&(l->b_thds), next, prev);
	/* FIXME: check for lock_id overflow */
	l->lock_id = lock_id++;
	l->owner = 0;
	l->gen_num = 0;
	l->spd = spd;
	INIT_LIST(l, next, prev);
	assert(&locks != l);
	snd = FIRST_LIST(&locks, next, prev);
	lst = LAST_LIST(&locks, next, prev);
	(l)->next = (&locks)->next;
	(l)->prev = (&locks); 
	(&locks)->next = (l); 
	(l)->next->prev = (l);
	assert(FIRST_LIST(&locks, next, prev) == l);
	assert(LAST_LIST(l, next, prev) == &locks);
	if (lst != &locks) {
		assert(LAST_LIST(&locks, next, prev) == lst);
		assert(FIRST_LIST(lst, next, prev) == &locks);
	}
	assert(FIRST_LIST(l, next, prev) == snd && LAST_LIST(snd, next, prev) == l);
	
//	lock_print_all();
	return l;
}

static void lock_free(struct meta_lock *l)
{
	assert(l && l != &locks);
	REM_LIST(l, next, prev);
	free(l);
}

/* Public functions: */

/* 
 * The problem being solved here is this: T_1 wishes to take the
 * mutex, finds that it is taken by another thread.  It calls into
 * this function, but is preempted by T_2, the lock holder.  The lock
 * is released.  T_1 is switched back to and it invokes this component
 * asking to block till the lock is released.  This component has no
 * way of knowing that the lock already has been released, so we block
 * for no reason in wait for the lock to be "released".  Thus what we
 * do is have the client call the pretake function checking before and
 * after invoking it that the lock is still taken.  We record the
 * generation number in pretake and make sure that it is consistent in
 * take.  This signifies that no release has happened in the interim,
 * and that we really should sleep.
 */
int lock_component_pretake(spdid_t spd, unsigned long lock_id, unsigned short int thd)
{
	struct meta_lock *ml;
 	spdid_t spdid = cos_spd_id();
	int ret = 0;

	/* printc("called in lcok_component_pretake \n"); */

	ACT_RECORD(ACT_PRELOCK, spd, lock_id, cos_get_thd_id(), thd);
	TAKE(spdid);

	WAIT_FAULT();
#if (RECOVERY_ENABLE == 1) && defined(TEST_LOCK_PRETAKE)
	/* printc("trigger fault in lock pre_take: thd %d spd %ld passed spd %d\n", cos_get_thd_id(), cos_spd_id(), spd); */
	/* if (spd == 16 && cos_get_thd_id() == 13 && test_num++ == 2) { */
	if (test_num++ > 20) {
		printc("trigger fault in lock pre_take: thd %d spd %ld passed spd %d\n", cos_get_thd_id(), cos_spd_id(), spd);
		assert(0);
	}
#endif

//	lock_print_all();
	ml = lock_find(lock_id, spd);
	if (NULL == ml) {
		/* printc("pretake: thd %d can not find ml for lock %ld spd %d\n",  */
		/*        cos_get_thd_id(), lock_id, spd); */
		ret = -EINVAL;
		goto done;
	}
	ml->gen_num = generation;
done:
	RELEASE(spdid);
	return ret;
}

/* 
 * Dependencies here (thus priority inheritance) will NOT be used if
 * you specify a timeout value.
 *
 * Return 0: lock taken, -1: could not find lock, 1: inconsistency -- retry!
 */
int lock_component_take(spdid_t spd, unsigned long lock_id, unsigned short int thd_id)
{
	struct meta_lock *ml;
	spdid_t spdid = cos_spd_id();
	unsigned short int curr = (unsigned short int)cos_get_thd_id();
	struct blocked_thds blocked_desc = {.thd_id = curr};
	int ret = -1;

	ACT_RECORD(ACT_LOCK, spd, lock_id, cos_get_thd_id(), thd_id);

	/* TAKE(spdid); */
	// Jiguo: see sched s_stub.S. Might need to release the lock
	// after the fault
	// TODO: move this to the server stub in scheduer
	if (sched_component_take(spdid) == -1) {
		if (sched_component_release(spdid)) assert(0);
	} 

	TAKE(spdid);

#ifdef BENCHMARK_MEAS_INV_OVERHEAD_LOCK
	if (cos_get_thd_id() == MEAS_LOCK_THREAD) goto error;
#endif

	WAIT_FAULT();

	/* while(swifi_ready == 1) { */
	/* 	if (tt++ < 1) printc("thread %d is spinning...\n", cos_get_thd_id()); */
	/* }  */
	/* printc("thread %d continues\n", cos_get_thd_id()); */
	
	/* printc("lock take before: thd %d spd %ld passed spd %d\n",  */
	/*        cos_get_thd_id(), cos_spd_id(), spd); */
	/* if (spd == 17 && cos_get_thd_id() == 14) while(1); */

#if (RECOVERY_ENABLE == 1) && defined(TEST_LOCK_TAKE_BEFORE)	
	/* printc("trigger fault in lock take before: thd %d spd %ld passed spd %d\n", cos_get_thd_id(), cos_spd_id(), spd); */
	/* if (spd == 17 && cos_get_thd_id() == 10 && test_num++ == 2) { */
	if (test_num++ > 5) {
		printc("trigger fault in lock take before: thd %d spd %ld passed spd %d\n", cos_get_thd_id(), cos_spd_id(), spd);
		assert(0);
	}
#endif

	ml = lock_find(lock_id, spd);
	/* tried to access a lock not yet created */
	if (!ml) {
		/* printc("lock id %lu goto error: can not find lock\n", lock_id); */
		ret = -EINVAL;
		goto error;
	}

	assert(!lock_is_thd_blocked(ml, curr));
	
	/* The calling component needs to retry its user-level lock,
	 * some preemption has caused the generation count to get off,
	 * i.e. we don't have the most up-to-date view of the
	 * lock's state */
	if (ml->gen_num != generation) {
		ml->gen_num = generation;
		ret = 1;
		/* printc("goto error 2\n"); */
		goto error;
	}
	generation++;

	/* Note that we are creating the list of blocked threads from
	 * memory allocated on the individual thread's stacks. */
	INIT_LIST(&blocked_desc, next, prev);
	ADD_LIST(&ml->b_thds, &blocked_desc, next, prev);
	//ml->owner = thd_id;

	RELEASE(spdid);

	/* printc("cpu %ld: thd %d going to blk waiting for lock %d\n",  */
	/*        cos_cpuid(), cos_get_thd_id(), (int)lock_id); */

	if (-1 == sched_block(spdid, thd_id)) {
		printc("Deadlock including thdids %d -> %d in spd %d, lock id %d.\n", 
		       cos_get_thd_id(), thd_id, spd, (int)lock_id);
		debug_print("BUG: Possible deadlock @ "); 
		assert(0);
		if (-1 == sched_block(spdid, 0)) assert(0);
	}

	WAIT_FAULT();
#if (RECOVERY_ENABLE == 1) && defined(TEST_LOCK_TAKE_AFTER)
	/* printc("trigger fault in lock take: thd %d spd %ld passed spd %d\n", cos_get_thd_id(), cos_spd_id(), spd); */
		
	/* if (spd == 17 && cos_get_thd_id() == 10 && test_num++ >= 0)  */
	if (test_num++ > 5) {
		printc("trigger fault in lock take after: thd %d spd %ld passed spd %d\n", cos_get_thd_id(), cos_spd_id(), spd);
		assert(0);
	}
#endif

	if (!EMPTY_LIST(&blocked_desc, next, prev)) {
		printc("lock_component_take: from spd %d thd %d found not empty blocked list\n",
		       spd, cos_get_thd_id());

		/* what if net thread is blocked, and another thread is woken
		 * up back to here? Will see a non-empty list? -- Jiguo */

		/* BUG(); */
	}

	/* 
	 * OK, this seems ridiculous but here is the rational: Assume
	 * we are a middle-prio thread, and were just woken by a low
	 * priority thread. We will preempt that thread when woken,
	 * and will continue here.  If a high priority thread is also
	 * waiting on the lock, then we would preempt the low priority
	 * thread while it should wake the high prio thread. With the
	 * following crit sect will switch to the low prio thread that
	 * still holds the component lock.  See the comments in
	 * lock_component_release. 
	 */
	//TAKE(spdid);
	//RELEASE(spdid);

	ACT_RECORD(ACT_WAKEUP, spd, lock_id, cos_get_thd_id(), 0);
	/* printc("lock_component_take ret 0\n"); */
	ret = 0;
done:
	return ret;
error:
	RELEASE(spdid);
	goto done;
}

int lock_component_release(spdid_t spd, unsigned long lock_id)
{
	struct meta_lock *ml;
	struct blocked_thds *sent, *bt;
	spdid_t spdid = cos_spd_id();

	int ret = -1;

	ACT_RECORD(ACT_UNLOCK, spd, lock_id, cos_get_thd_id(), 0);
	TAKE(spdid);

	generation++;
	ml = lock_find(lock_id, spd);
	if (!ml) goto error;

	WAIT_FAULT();
	/* int tt = 0; */
	/* while(swifi_ready == 1) { */
	/* 	if (tt++ < 1) printc("thread %d is spinning...\n", cos_get_thd_id()); */
	/* }  */
	/* printc("thread %d continues\n", cos_get_thd_id()); */

#if (RECOVERY_ENABLE == 1) && defined(TEST_LOCK_RELEASE_BEFORE)
	/* printc("trigger fault in lock release: thd %d spd %ld passed spd %d\n", */
	/*        cos_get_thd_id(), cos_spd_id(), spd); */
	/* if (spd == 16 && cos_get_thd_id() == 13 && test_num++ == 2) { */
	if (test_num++ > 5) {
		printc("trigger fault in lock release before: thd %d spd %ld passed spd %d\n",
		       cos_get_thd_id(), cos_spd_id(), spd);
		assert(0);
	}
#endif

	/* Apparently, lock_take calls haven't been made. */
	if (EMPTY_LIST(&ml->b_thds, next, prev)) {
		RELEASE(spdid);
		return 0;
	}
	sent = bt = FIRST_LIST(&ml->b_thds, next, prev);
	/* Remove all threads from the lock's list */
	REM_LIST(&ml->b_thds, next, prev);
	/* Unblock all waiting threads */
	while (1) {
		struct blocked_thds *next;
		u16_t tid;

		/* This is suboptimal: if we wake a thread with a
		 * higher priority, it will be switched to.  Given we
		 * are holding the component lock here, we should get
		 * switched _back_ to so as to wake the rest of the
		 * components. */
		next = FIRST_LIST(bt, next, prev);
		REM_LIST(bt, next, prev);

		ACT_RECORD(ACT_WAKE, spd, lock_id, cos_get_thd_id(), bt->thd_id);

		/* cache locally */
		tid = bt->thd_id;
		/* Last node in the list? */
		if (bt == next) {
			/* This is sneaky, so to reiterate: Keep this
			 * lock till now so that if we wake another
			 * thread, and it begins execution, the system
			 * will switch back to this thread so that we
			 * can wake up the rest of the waiting threads
			 * (one of which might have the highest
			 * priority).  We release before we wake the
			 * last as we don't really need the lock
			 * anymore, an it will avoid quite a few
			 * invocations.*/
			RELEASE(spdid);
		}

		/* Wakeup the way we were put to sleep */
		assert(tid != cos_get_thd_id());
		/* printc("CPU %ld: %d waking up %d for lock %d\n", cos_cpuid(), cos_get_thd_id(), tid, lock_id); */
		sched_wakeup(spdid, tid);

		WAIT_FAULT();
#if (RECOVERY_ENABLE == 1) && defined(TEST_LOCK_RELEASE_AFTER)
		/* printc("trigger fault in lock release: thd %d spd %ld passed spd %d\n", */
		/*        cos_get_thd_id(), cos_spd_id(), spd); */
		/* if (spd == 16 && cos_get_thd_id() == 13 && test_num++ == 2) { */
		if (test_num++ > 5) {
			printc("trigger fault in lock release after: thd %d spd %ld passed spd %d\n",
			       cos_get_thd_id(), cos_spd_id(), spd);
			assert(0);
		}
#endif

		if (bt == next) break;
		bt = next;
	}

	ret = 0;
done:	
	RELEASE(spdid);
	return ret;
error:
	ret = -EINVAL;
	goto done;
}

unsigned long lock_component_alloc(spdid_t spd)
{
	struct meta_lock *l;
	spdid_t spdid = cos_spd_id();

	/* printc("lock_component_alloc 1\n"); */
	TAKE(spdid);
	/* printc("lock_component_alloc 1.1.....\n"); */
	l = lock_alloc(spd);
	/* printc("lock_component_alloc 2\n"); */
	WAIT_FAULT();
#if (RECOVERY_ENABLE == 1) && defined(TEST_LOCK_ALLOC)
	/* printc("trigger fault in lock alloc: thd %d spd %ld passed spd %d\n", cos_get_thd_id(), cos_spd_id(), spd); */
	if (spd == 17 && test_num++ == 2) {
	/* if (test_num++ == 5) { */
		printc("trigger fault in lock alloc: thd %d spd %ld passed spd %d\n", cos_get_thd_id(), cos_spd_id(), spd);
		assert(0);
	}
#endif

	RELEASE(spdid);
	/* printc("lock_component_alloc 3\n"); */
	if (!l) return 0;
	/* printc("lock_component_alloc id %lu(request by spd %d)\n", l->lock_id, spd); */
 	return l->lock_id;
}

int lock_component_free(spdid_t spd, unsigned long lock_id)
{
	struct meta_lock *l;
	spdid_t spdid = cos_spd_id();

	if (sched_component_take(spdid)) return 0;
	l = lock_find(lock_id, spd);
	if (sched_component_release(spdid)) return 0;

	if (l) lock_free(l);

	return 0;
}

#ifdef ACT_LOG
unsigned long *lock_stats(spdid_t spdid, unsigned long *stats)
{
	struct action *a;
	int sz = (NUM_ACT_ITEMS + 2) * sizeof(unsigned long);

	if (!cos_argreg_buff_intern((char*)stats, sz)) {
		return NULL;
	}
	
	if (NULL == (a = action_report())) return NULL;
	memcpy(stats, a, sz);
	return stats;
}

int lock_stats_len(spdid_t spdid)
{
	return NUM_ACT_ITEMS + 2;
}
#else 

unsigned long *lock_stats(spdid_t spdid, unsigned long *stats) { return NULL; }
int lock_stats_len(spdid_t spdid) { return 0; }

#endif

static int rec_thd = 0;
static int first = 0;

static void rd_reflection();

void cos_init()
{
	union sched_param sp;

	if (cos_get_thd_id() == rec_thd) {
		printc("rec thread %d is in spd %ld\n", cos_get_thd_id(), cos_spd_id());
		return;
	}
	
	if (first == 0) {
		first = 1;
		/* printc("before init_static thd %d\n", cos_get_thd_id()); */
		cos_vect_init_static(&bthds);
		/* printc("after init_static thd %d\n", cos_get_thd_id()); */

		// Jiguo: rdreflection
		if (unlikely(cos_fault_cntl(COS_CAP_REFLECT_UPDATE, cos_spd_id(), 0))) {
			/* printc("to create a recovery thread\n"); */
			/* sp.c.type = SCHEDP_PRIO; */
			/* printc("2\n"); */
			/* sp.c.value = 3; */
			/* printc("3\n"); */
			/* rec_thd = sched_create_thd(cos_spd_id(), sp.v, 0, 0); */
			/* printc("creates a recovery thread %d @ prio 3\n", rec_thd); */
			
			/* printc("\nneed do reflection now!!!!\n"); */
			rd_reflection();
			/* printc("\nreflection done!!!\n\n"); */
		}
	}
}

extern int sched_reflect(spdid_t spdid);
extern int sched_wakeup(spdid_t spdid, unsigned short int thd_id);
extern vaddr_t mman_reflect(spdid_t spd, int src_spd, int cnt);
extern int mman_release_page(spdid_t spd, vaddr_t addr, int flags); 

static void
rd_reflection()
{
	int count_obj = 0; // reflected objects
	int dest_spd = cos_spd_id();
	
	/* // remove the mapped page for lock spd */
	/* vaddr_t addr; */
	/* count_obj = mman_reflect(cos_spd_id(), dest_spd, 1); */
	/* /\* printc("lock relfects on mmgr: %d objs\n", count_obj); *\/ */
	/* while (count_obj--) { */
	/* 	addr = mman_reflect(cos_spd_id(), dest_spd, 0); */
	/* 	/\* printc("lock mman_release: %p addr\n", (void *)addr); *\/ */
	/* 	assert(addr); */
	/* 	memset((void *)addr, 0, PAGE_SIZE);  // zero out anything on this page before release */
	/* 	mman_release_page(cos_spd_id(), addr, dest_spd); */
	/* } */

	sched_client_fault_notification(cos_spd_id());
	// to reflect all threads blocked from lock component
	/* int wake_thd; */
	/* count_obj = sched_reflect(cos_spd_id(), dest_spd, 1); */
	/* printc("lock relfects on sched: %d objs\n", count_obj); */

	/* while (count_obj--) { */
	/* 	wake_thd = sched_reflect(cos_spd_id(), dest_spd, 0); */
	/* 	/\* printc("wake_thd %d\n", wake_thd); *\/ */
	/* 	sched_wakeup(cos_spd_id(), wake_thd); */
	/* } */

	printc("lock reflection done (thd %d)\n\n", cos_get_thd_id());

	return;
}

int lock_reflect(spdid_t spdid) {return 0;}

void lock_client_fault_notification(spdid_t spdid) {return;}

void cos_upcall_fn(upcall_type_t t, void *arg1, void *arg2, void *arg3)
{
	/* printc("LOCK: upcall type %d, core %ld, thd %d, args %p %p %p\n", */
	/*        t, cos_cpuid(), cos_get_thd_id(), arg1, arg2, arg3); */
	switch (t) {
	case COS_UPCALL_THD_CREATE:
	{
		cos_init();
		break;
	}
	case COS_UPCALL_SWIFI_BEFORE:   // prepare for swifi
	{
		swifi_ready = 1; // now other thread can spin for inject fault
		break;
	}
	case COS_UPCALL_SWIFI_AFTER:   // allow other thread to proceed 
	{
		swifi_ready = 0; // now other thread can continue
		break;
	}
	case COS_UPCALL_RECOVERY:
	{
		printc("thread %d passing arg1 %p here (type %d spd %ld) to recover parent\n",
		       cos_get_thd_id(), arg1, t, cos_spd_id());
		mem_mgr_cli_if_recover_upcall_entry((vaddr_t)arg1);
		break;
	}
	case COS_UPCALL_RECOVERY_SUBTREE:
	{
		printc("thread %d passing arg1 %p here (type %d spd %ld) to recover subtree\n",
		       cos_get_thd_id(), arg1, t, cos_spd_id());
		mem_mgr_cli_if_recover_upcall_subtree_entry((vaddr_t)arg1);
		break;
	}
	case COS_UPCALL_REMOVE_SUBTREE:
	{
		printc("thread %d passing arg1 %p here (type %d spd %ld) to remove subtree\n",
		       cos_get_thd_id(), arg1, t, cos_spd_id());
		mem_mgr_cli_if_remove_upcall_subtree_entry((vaddr_t)arg1);
		break;
	}
	default:
		return;
	}
	return;
}
